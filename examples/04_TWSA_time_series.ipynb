{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/IISc_Master_Seal_Black_Transparent.png\" height=\"120px\" width=\"120px\" align=\"right\" />\n",
    "\n",
    "<img src=\"imgs/logoGESS.jpg\" height=\"120px\" width=\"120px\" align=\"right\" />\n",
    "\n",
    "<font face=\"Calibri\">\n",
    "<font size=\"7\"> <b> PySHBundle Tutorials </b> </font>\n",
    "<br> \n",
    "<font size=\"5\"> <b> Tutorial 4: Terrestrial Water Storage Time Series<font color='rgba(200,0,0,0.2)'>  </font> </b> </font>\n",
    "<br> <br>\n",
    "    \n",
    "<font size=\"3\"> <b> by: Abhishek Mhamane<sup>1</sup>, Vivek Kumar Yadav<sup>2</sup> <br>\n",
    "<sup>1</sup>IIT Kanpur, <sup>2</sup>ICWaR, IISc Bengaluru</b> \n",
    "<font size=\"2\">  <br>\n",
    "<font> <b>Date: </b> June 07, 2024 </font>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing GRACE level 2 spherical harmonics data into terrestrial water storage anomalies (TWSA)\n",
    "\n",
    "This notebook explains each step of getting TWSA field, followed by the process itself.\n",
    "\n",
    "It is divided into three sections,\n",
    "* A. Acquiring corrected GRACE level 2 spherical harmonics data.\n",
    "* B. Processing (A) into global TWSA anomaly field.\n",
    "* C. (Additional) Clipping TWSA for a particular basin and calculating basin average TWSA time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **A**. Acquiring corrected GRACE level 2 spherical harmonics data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.1 Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "# Add the folder path to the Python path\n",
    "folder_path = '../pyshbundle/'\n",
    "sys.path.append(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyshbundle.hydro import TWSCalc\n",
    "from pyshbundle.io import extract_SH_data, extract_deg1_coeff_tn13, extract_deg2_3_coeff_tn14\n",
    "from pyshbundle.viz_utils import plot_calendar_months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2 Specifying the path to input data.\n",
    "As part of the package we provide the input data from 3 centres `JPL`, `CSR` & `ITSG` and the corresponding (recommended) Technical notes (`TN13`, `TN14`) files as well. User may download there own solutions and provide their paths below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source='jpl'\n",
    "path_sh = \"../sample_input_data/JPL_input/\"\n",
    "# source='csr'\n",
    "# path_sh = \"../sample_input_data/CSR_input/\"\n",
    "# source='itsg'\n",
    "# path_sh = \"../sample_input_data/ITSG_input/\"\n",
    "\n",
    "path_tn14 = \"../pyshbundle/data/JPL_TN_files/TN-14_C30_C20_GSFC_SLR.txt\"    # Path to TN14\n",
    "path_tn13 = \"../pyshbundle/data/JPL_TN_files/TN-13_GEOC_JPL_RL06.txt\"       # Path to TN13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(path_sh)\n",
    "if str.upper(source) == 'ITSG':\n",
    "    file_paths = [path_sh + file for file in files if os.path.splitext(file)[1] == '.gfc'];\n",
    "else:\n",
    "    file_paths = [path_sh + file for file in files if os.path.splitext(file)[1] == '.gz'];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A3. Process each file and store the data in the main dictionary\n",
    "Dictionary to store the extracted data, the data can be accessed using the standard format of any python dictionary with the *key-value* pair. Access using `extracted_data['yyyy-mm'][(degree, order)]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data={} \n",
    "for file_path in file_paths:\n",
    "    # file_data = read_sh(file_path, source=source)\n",
    "    file_data = extract_SH_data(file_path, source=source)\n",
    "    if file_data['time_coverage_start']:\n",
    "        # Convert time_coverage_start to a datetime object and then format it as yyyy-mm\n",
    "        if source == 'itsg':\n",
    "            start_date = datetime.strptime(file_data['time_coverage_start'][-7:], '%Y-%m').strftime('%Y-%m')\n",
    "        else:\n",
    "            start_date = datetime.strptime(file_data['time_coverage_start'], '%Y-%m-%dT%H:%M:%S.%f').strftime('%Y-%m')\n",
    "        # Use the formatted date as the key\n",
    "        extracted_data[start_date] = file_data['coefficients']\n",
    "\n",
    "\n",
    "# Time Sort the dictionary by keys (dates)\n",
    "sorted_data = OrderedDict(sorted(extracted_data.items()));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lets quickly visualize which month and years of data you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_object = sorted_data.keys()\n",
    "\n",
    "plot_calendar_months(datetime_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A4. Nested *dictionary* structure of the extracted and time sorted data\n",
    "The keys of the `sorted_data` dictionary are the dates in the format `(yyyy-mm)`. \n",
    "\n",
    "Each key as a dictionary for a particular `(yyyy-mm)` pair has list of dictionaries for each `(degree, order)` pair that was read from the data, see example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(sorted_data.keys())[0:5], '\\n')             # dicitonary for each year-month, printing only first 5\n",
    "print(list(sorted_data['2002-04'].keys())[0:5], '\\n')  # dictionary for each degree and order, , printing only first 5\n",
    "print(sorted_data['2002-04'][(2,0)].keys(), '\\n')      # dictionary with the data for repective year-month, degree and order\n",
    "print('Clm value for (degree, order)=(2,0) is:', sorted_data['2002-04'][(2,0)]['Clm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A5. Replacement of degree 2,3 coefficients\n",
    "Since the degree 2 & 3 (order 0) coefficients from GRACE are errorneous, these need to be replaced with the corresponding data from other sources. \n",
    "\n",
    "The recommended method is using Satellite laser ranging data. As mentioned above, we do provide the `TN14` data (JPL, CSR, & ITSG) with the package. Since this data is continuously updated users may download there own solutions for the same and use for replacement of `C20` and `C30` coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A.5.1 Reading data from TN14 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_tn14 = extract_deg2_3_coeff_tn14(path_tn14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Once again we use python dictionary to store the replacement `C20`, `C30` coefficients for each `yyyy-mm`. They can be easily accessed as below.\n",
    "\n",
    "* Note that lot of the starting `C30` values are `None`, since these did not need replacement in the initial years and hence are defined in `TN14` as `None` itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Keys of the TN14 replacement dictionary', list(temp_tn14.keys())[0:5],)        # dicitonary for each year-month, , printing only first 5\n",
    "print('Keys of dictionary for year-month 2002-4', temp_tn14['2002-04'].keys())  # dictionary for c20, c30 coefficients for each year-month\n",
    "print('For 2002-04 :',  '\\n'\n",
    "      'C20 is:', temp_tn14['2002-04']['c20'], '\\n'\n",
    "      'C30 is:',temp_tn14['2002-04']['c30'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A.5.2 Replacing C20 values in `sorted_data` with the values from `temp_tn14` dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date_key in temp_tn14.keys():\n",
    "    if date_key in sorted_data.keys():\n",
    "        sorted_data[date_key][(2,0)]['Clm'] = temp_tn14[date_key]['c20']\n",
    "        if temp_tn14[date_key]['c30'] is not None:\n",
    "            sorted_data[date_key][(3,0)]['Clm'] = temp_tn14[date_key]['c30'];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.6 Replacement of degree 1 coefficients\n",
    "\n",
    "Since GRACE does not provide degree 1 coeffieicents, we need to replace (insert) them to get the complete spherical harmonics data starting from (degree, order) [0,0] to [$l_{max}$, $m_{max}$]\n",
    "\n",
    "The degree 1 coefficients are provided by various centres and are called Techinical notes 13. Similar to TN14, Pyshbundle includes TN13 files from JPL, CSR & ITSG. Users are suggested to download there own solutions for use since the TN13 & TN14 data in pyshbundle will not be updated frequently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A.6.1 Reading data from TN13 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_tn13=extract_deg1_coeff_tn13(path_tn13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The data from TN13 is again saved with a *key-value* pair for each `year-month`. As mentioned above, the coefficients we get from TN13 are `C10`, `S10`, `C11`, `S11`. It can be accessed as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Keys of the TN13 replacement dictionary', list(temp_tn13.keys())[0:5],)        # dicitonary for each year-month, printing only first 5\n",
    "print('Keys of dictionary for year-month 2002-4', temp_tn13[('2002-04', 1, 0)].keys())  # dictionary for c20, c30 coefficients for each year-month\n",
    "print('For 2002-04 :',  '\\n'\n",
    "       'C10 is:', temp_tn13[('2002-04', 1, 0)]['Clm'], '\\n'\n",
    "       'S10 is:', temp_tn13[('2002-04', 1, 0)]['Slm'], '\\n'\n",
    "       'C11 is:', temp_tn13[('2002-04', 1, 1)]['Clm'], '\\n'\n",
    "       'C11 is:', temp_tn13[('2002-04', 1, 1)]['Slm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since **JPL** solutions do not provide degree 1 coeffs (`C10`, `C11`, `S10`, `S11`) new dictionary key-value pairs need to be initialized for them with 0 values. We shall replace them in the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "if str.upper(source)=='JPL':\n",
    "    for key in sorted_data:\n",
    "        sorted_data[key][(0, 0)] = {'Clm': 0.0, 'Slm': 0.0, 'Clm_sdev': 0.0, 'Slm_sdev': 0.0}\n",
    "        sorted_data[key][(1, 0)] = {'Clm': 0.0, 'Slm': 0.0, 'Clm_sdev': 0.0, 'Slm_sdev': 0.0}\n",
    "        sorted_data[key][(1, 1)] = {'Clm': 0.0, 'Slm': 0.0, 'Clm_sdev': 0.0, 'Slm_sdev': 0.0};\n",
    "else: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A.6.2 Replacing degree 1 values in `sorted_data` with the values from `temp_tn13` dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date_key in temp_tn13.keys():\n",
    "    if date_key[0] in sorted_data.keys():\n",
    "        # print(date_key)\n",
    "        sorted_data[date_key[0]][(date_key[1], date_key[2])]['Clm'] = temp_tn13[date_key]['Clm']\n",
    "        sorted_data[date_key[0]][(date_key[1], date_key[2])]['Slm'] = temp_tn13[date_key]['Slm']\n",
    "        sorted_data[date_key[0]][(date_key[1], date_key[2])]['Clm_sdev'] = temp_tn13[date_key]['Clm_sdev']\n",
    "        sorted_data[date_key[0]][(date_key[1], date_key[2])]['Slm_sdev'] = temp_tn13[date_key]['Slm_sdev'];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how many months of data we have and what is the maximum degree ($l_{max}$) and order ($m_{max}$) present in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_degree=np.max([degree for date in sorted_data.keys() for degree, order in sorted_data[date].keys()])\n",
    "max_order=np.max([order for date in sorted_data.keys() for degree, order in sorted_data[date].keys()])\n",
    "number_of_months=len(sorted_data.keys())\n",
    "print('The maximum degree & order in data is:' , max_degree, '&', max_order)\n",
    "print('Number of months for which data is available:', number_of_months)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A.7 Processing level 2 spherical harmonics data\n",
    "\n",
    "Depending on the use, GRACE spherical harmonics data is stored in various formats such as `/S|C\\`, `C|S` and `KLM`. The two figures below show the most common formats.\n",
    "/Users/vivek/jupyter_nbs/gitrepos/pyshbundle/notebooks/imgs/sc_matrix.png\n",
    "<center>\n",
    "<img alt=\"CS matrix\" src=\"./imgs/cs_matrix.png\" width=\"500\" height=\"300\">\n",
    "<center>\n",
    "<img alt=\"SC matrix\" src=\"./imgs/sc_mat.png\" width=\"700\" height=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.7.1 Lets save the data in `/S|C\\` format\n",
    "\n",
    "* Now that we are done with accquring the data and replaced erroneous and missing coefficients. Lets store the SH coeffiecients in `clm` format.\n",
    "\n",
    "* Since the `sorted_data` has the (degree, order) pair as a key for each month, we can access the coefficients using those itself. Below is a simple `for` loop which implements it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_mat=np.zeros([number_of_months, max_degree+1, 2*(max_degree+1)], dtype=np.double)\n",
    "\n",
    "for index, key in enumerate(sorted_data.keys()):\n",
    "    temp=sorted_data[key]\n",
    "    for l in range(0,max_degree+1):\n",
    "        for m in range(0,l+1):\n",
    "            '''uncomment these two lines to see how the elements are being accessed from the dictionary'''\n",
    "            # print(l,m)\n",
    "            # print(temp[(l,m)]['Clm'])\n",
    "            sc_mat[index, l, max_degree+m+1]=temp[(l,m)]['Clm']\n",
    "            sc_mat[index, l, max_degree-m]=temp[(l,m)]['Slm']\n",
    "    del temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since all order 0 coefficients of $S_{lm}$ are 0, we can delete that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_mat=np.delete(sc_mat, max_degree, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **B**. Processing (A) into global TWSA anomaly fields.\n",
    "\n",
    "Now that we have obtained the corrected the GRACE spherical harmonics coefficients and stored them in a suitable format (`/S|C\\`) we shall now begin the actual processing. The steps that follow will decide the interpretation (with respect to a mean) and spatial resolution of the final solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B.1 Removing a temporal mean to get `delta_SC` matrix\n",
    "\n",
    "* Since the exact density data of the Earth is unkown, a mean gravitational state of the Earth has to be removed.\n",
    "\n",
    "* This can be done by taking a mean of the spherical harmonics coefficients over a period. The most common choice is taking a mean over the period of January 2003 to December 2010 since there is no gap in this period. However user may choose a different period for taking the mean or simply take a mean over the entire period by commenting out the line below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SH_long_mean_jpl = np.load('../pyshbundle/data/long_mean/SH_long_mean_jpl.npy')    # load the long term mean SH coeffs---> JPL \n",
    "SH_long_mean_csr = np.load('../pyshbundle/data/long_mean/SH_long_mean_csr.npy')    # load the long term mean SH coeffs---> CSR\n",
    "SH_long_mean_itsg = np.load('../pyshbundle/data/long_mean/SH_long_mean_itsg.npy')    # load the long term mean SH coeffs---> ITSG\n",
    "delta_sc=np.ones_like(sc_mat)*np.nan\n",
    "if source== 'jpl':\n",
    "    delta_sc = sc_mat -   SH_long_mean_jpl\n",
    "    # delta_sc = sc_mat -   sc_mat.mean(axis=0)      # Remove the mean of the data\n",
    "elif source== 'csr':\n",
    "    delta_sc = sc_mat - SH_long_mean_csr\n",
    "elif source== 'itsg':\n",
    "    delta_sc = sc_mat - SH_long_mean_itsg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "scipy.io.savemat('/Users/vivek/Desktop/vivek_desktop/test/delta_sc_mat_float64.mat', {'del_sc_mat': delta_sc})\n",
    "# np.save('/Users/vivek/Desktop/vivek_desktop/test/sc_mat_float64.npy', sc_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B2. $L_{max}$, Gaussian half radius and grid resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $L_{max}$\n",
    "   \n",
    "  Now we choose the truncation limit of the spherical harmonics expansion.\n",
    "  \n",
    "  Know that the upper limit of this expansion is the maximum degree coefficient present in your data. \n",
    "  Thus the highest limit can be $l_{max}$. The higher the limit of truncation, the higher the spatial resolution of the solutions, the trade-off being increasing levels of noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Gaussian half radius\n",
    "\n",
    "  Since the GRACE spherical harmonics coefficients are extremely nosiy, certain levels of filtering are required to reduce the same.\n",
    "\n",
    "  The simplest form of filtering is applying a Gaussian filter. More advance methods of filtering do exist such as `etc` as well as dynamic filters such as DDK, DDK5.\n",
    "\n",
    "  We provide Gaussian filtering, the filter can be imagined as a symmetrical hat moving (convolution) over the surface of the sphere (Earth in our case), weighting the values under it. The extent of surrounding weighting is decided by the radius of the filter. The values can be calculated using the `Gaussian()` function. The function takes in the maximum degree to which the filterhas to be created for, and the radius of the Gaussian function. Since a Gaussian function is a symmterical function, the function only returns only one half of the values. This can be seen below in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyshbundle.shutils import Gaussian\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=Gaussian(L=40, cap=500);\n",
    "plt.plot(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Grid resolution\n",
    "  \n",
    "  The resolution of the gridded TWSA fields to which the global spherical harmonics synthesis (`GSHS`) applies is needed to be decided by the user. Depending on their use, they may choose any positive integer for the grid size.\n",
    "\n",
    "  Users should take note that choosing a finer grid size does not mean a higher spatial resolution of the data since that is mathematically contrained by the limit of truncation ($L_{max}$) of the spherical harmoics function (as mentioned above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Kindly enter the following parameters based on your application')\n",
    "# lmax = int(input(\"Enter lmax: \"))\n",
    "# gs = int(input(\"Enter grid size: \"))\n",
    "# half_rad_gf = int(input(\"Enter half radius of Gaussian filter: \"))\n",
    "# print(\"lmax , grid size & half raduius of gaussian filter are : \", lmax ,\", \" , gs,\", \",half_rad_gf)\n",
    "lmax,gs,half_rad_gf=96, 1, 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B3. Global spherical harmonics synthesis to get gridded global TWSA data.\n",
    "\n",
    "Now that we have decided the choices of processing, we shall now do the actual processing.\n",
    "\n",
    "The `TWSCalc` function does thre things:\n",
    "* Calls the `Gaussian` function to generate the filtering values.\n",
    "* Applies the gaussian filter on the `delta_SC` matrix.\n",
    "* Converts the filtered `delta_SC` matrix to gridded global TWSA field using the `GSHS` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tws_fields = TWSCalc(delta_sc,lmax, gs,half_rad_gf, number_of_months)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision required for the calculations uptill now was of very high order, since the coefficients are of order `1e-12`, with all digits significant. But now we can reduce the precision since the generated TWSA values are of much higher order and lesser singificant digits after (3) decimal. This reduces the size of the dataset and saves system memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tws_fields = np.float32(tws_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B4. Creating a netcdf dataset\n",
    "\n",
    "Netcdf is a popular format for sharing climate & hydrological data, let us create a netcdf dataset of our gridded results for easier analysis and sharing.\n",
    "\n",
    "A global gridded netcdf file needs three coordinates, latitude, longitude and time for the data. We can create the latitude and longitude coordinates using the grid size (`gs`) decided above. The time information can be extracted from the keys of the `sorted_data` dictioanry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon = np.arange(-180,180,gs)\n",
    "lat = np.arange(89,-91,-gs)\n",
    "dates = pd.to_datetime(list(sorted_data.keys()), format='%Y-%m',) \\\n",
    "            + pd.offsets.MonthEnd(0)  #.dt.strftime('%d-%m-%Y')\n",
    "\n",
    "ds = xr.Dataset(\n",
    "    data_vars=dict(\n",
    "        tws=([\"time\",\"lat\", \"lon\"], tws_fields, {\"units\": \"mm\"})\n",
    "    ),\n",
    "    coords = {\n",
    "        # \"time\":(('time'),dates),\n",
    "        \"time\":dates,\n",
    "        \"lat\":lat,\n",
    "        \"lon\":lon },\n",
    "    attrs=dict(description=\"Global gridded TWS Anomaly corresponding to long term (2004-2010) mean\", \n",
    "               units=\"mm\"),\n",
    ")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_netcdf('/Users/vivek/Desktop/vivek_desktop/test/ds_py_float64.nc', engine='netcdf4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['tws'].isel(time=-1).plot(figsize=(8, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **C**. Clipping TWSA for a particular basin and calculating basin average TWSA time series.\n",
    "#### (Additional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For basin-scale analysis such as closing the water budget.\n",
    "\n",
    "$$\n",
    "\\frac{dS_t}{dt} = P + R +ET\n",
    "$$\n",
    "\n",
    "One needs to calculate the change in storage over the entire basin. This can be done by clipping the above global dataset within the region of interest (typically a basin or area shapefile).\n",
    "\n",
    "Below we use commonly used python packages shapely, gepopandas and rioxarray to clip data from a shapefile for the Ganges basin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C1. Selection of basin or shapefile in region of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The pyshbundle includes the shapefiles from of the major river basins across the world, obtained from the **HydroBASINS** project `mrb` (major river basins).\n",
    "\n",
    "* Note that user may find their basin of analysis from the `mrb` as shown below or load their own shapefile and save it in the `shp_basin` variable and its area in the `basin_area` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_shapefile = '../pyshbundle/data/mrb_shapefiles/mrb_basins.shp'\n",
    "shp = gpd.read_file(path_shapefile)\n",
    "shp.plot(figsize=(8, 4))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Find your basin from the `shp` geodataframe. \n",
    "  \n",
    "  My choice of basin is Ganga basin in the Asian subcontinent.\n",
    "\n",
    "  I subset the large `shp` geodataframe for the 'Asia' continet and inspect the `temp` variable for the Ganga basin, note that sometimes there maybe difference in the spelling of a certain basin's name. Which is the case with the Ganga basin being spelled as \"GANGES\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=shp['RIVER_BASI'].where(shp['CONTINENT']=='Asia').dropna()\n",
    "print(temp)\n",
    "del temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_name='GANGES'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we will extract the shapefile of the basin selected above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shp_basin=shp[shp['RIVER_BASI']==basin_name]\n",
    "print(shp_basin.head(), '\\n')\n",
    "shp_basin.plot()\n",
    "basin_area=np.float64(shp_basin['SUM_SUB_AR'].values[0])*1e6          # basin area already in m^2\n",
    "print('Basin area is :', basin_area, 'm\\u00b2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C2. Area weighting\n",
    "\n",
    "* Before clipping the data inside the shapefile, the dataset has to be area weighted according to the area of the grid cell. This is due to Earth's spherical shape and size of grids reduces as one moves from the equator towards the poles. This becomes pertinent if the region of interest is spread across several latitudes.\n",
    "\n",
    "* Thus we area weight our dataset by multiplying the area of each grid with the corresponding grid value. Then we take an average over that area by summing up the values inside the shapefile and dividing by the area of the shapefile (`basin_area`).\n",
    "\n",
    "* `area_weighting` function generates a matrix with area of each grid at each latitude and longitude box. Once again we use the `gs` variable to create the global area matrix. The plot below shows how the area decreases from the equator to the poles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyshbundle.hydro import area_weighting\n",
    "temp = area_weighting(gs)\n",
    "lons, lats = np.meshgrid(np.linspace(-180, 180, temp.shape[1]), np.linspace(-90, 90, temp.shape[0]))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4), subplot_kw={'projection': ccrs.Orthographic()})\n",
    "cax = ax.pcolormesh(lons, lats, temp, cmap='viridis', transform=ccrs.PlateCarree())\n",
    "\n",
    "# Add a colorbar with a label for clarity\n",
    "cbar = fig.colorbar(cax, ax=ax, orientation='vertical', pad=0.07)\n",
    "cbar.set_label('Area ($m^2$)', rotation=270, labelpad=15, fontsize=10)\n",
    "\n",
    "ax.set_ylabel('Latitude', fontsize=10)\n",
    "ax.set_xlabel('Longitude', fontsize=10)\n",
    "ax.gridlines(draw_labels=True)\n",
    "plt.title('Global Area Matrix Showing Area of Each Grid Cell in $m^2$', fontsize=11, pad=20)\n",
    "plt.show()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Clip data using shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyshbundle.hydro import Basinaverage\n",
    "basin_tws, basin_avg_tws = Basinaverage(ds, gs, shp_basin, basin_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_avg_tws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 4))\n",
    "basin_avg_tws['tws'].plot()\n",
    "plt.grid(alpha=0.3)\n",
    "fig.suptitle(f'TWSA for basin: {basin_name}', fontsize=11)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C4. Adding gaps in the data\n",
    "\n",
    "The dataset we have created does not yet have the gaps in the time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C.4.1. First lets create a gapped dataset with NaN values. We will then fill the data that we have and leave the gaps as NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a new time index with monthly frequency for the gapped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dates=pd.date_range(start=basin_avg_tws.time[0].values, \n",
    "                        end=basin_avg_tws.time[-1].values, freq='ME',)\n",
    "new_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initializing an empty dataset for the gapped dataset, `basin_avg_tws_gapped`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_avg_tws_gapped = xr.Dataset(\n",
    "        data_vars = dict(   tws=([\"time\"], np.nan*np.arange(len(new_dates)))),\n",
    "        coords=dict(time=new_dates),)\n",
    "basin_avg_tws_gapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C.4.2. Now fill the data for the months where data is available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can use the `.isin()` method to check if the time values in `basin_avg_tws` are present in `basin_avg_tws_gapped`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_avg_tws_gapped['time'].isin(basin_avg_tws['time'])  # shows where indices where the dates match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can used to fill the values in the gapped dataset (`basin_avg_tws_gapped`) that was intialized for the dates where the data is available (`basin_avg_tws`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_avg_tws_gapped['tws'] = basin_avg_tws['tws'].where(\n",
    "    basin_avg_tws['time'].isin(basin_avg_tws_gapped['time']),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_avg_tws_gapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Lets see how the data looks now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 4))\n",
    "basin_avg_tws['tws'].plot()\n",
    "basin_avg_tws_gapped['tws'].plot()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend(['Original', 'Gapped'])\n",
    "plt.suptitle(f'GRACE TWSA anomaly in {basin_name} (with gaps)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Same can be done quickly with the original dataset `ds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an empty gapped dataset\n",
    "ds_gapped = xr.Dataset(\n",
    "    data_vars=dict(\n",
    "        tws=([\"time\",\"lat\", \"lon\"], np.nan * np.ones(   [len(new_dates), len(lat), len(lon)]   )     )  ),\n",
    "    coords = {\n",
    "        \"time\":new_dates,\n",
    "        \"lat\":lat,\n",
    "        \"lon\":lon },\n",
    "    attrs=dict(description=\"TWS Anomaly corresponding to long term (2004-2010) mean \\n \\\n",
    "               data contains missing values for the months where GRACE data is unavailable\"),    )\n",
    "\n",
    "# Filling the gaps where data is available\n",
    "ds_gapped['tws'] = ds['tws'].where(     ds['time'].isin(ds_gapped['time']),     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_gapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lets have a glance at the complete dataset now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=(ds_gapped['time'].shape[0]//10+1), ncols=10, figsize=(10, 20))\n",
    "ax_flat = ax.flatten()\n",
    "for timeindex, index in enumerate(ds_gapped.time.values):\n",
    "    ax_flat[timeindex].imshow(ds_gapped['tws'].sel(time=index), cmap='RdBu_r', vmin=-200, vmax=300)\n",
    "    ax_flat[timeindex].set_title(ds_gapped['time'].isel(time=timeindex).dt.strftime('%Y-%m').values)\n",
    "    ax_flat[timeindex].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds.to_netcdf('../pyshbundle/validation_data/tws_py.nc')\n",
    "# delta_sc.shape\n",
    "# import scipy\n",
    "\n",
    "# scipy.io.savemat('../pyshbundle/delta_sc.mat', {'delta_sc': delta_sc})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Want to save the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save the gridded data\n",
    "save_path_gridded = ''\n",
    "# Path to save the basin data\n",
    "save_path_basin = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_gapped.to_netcdf(f'{save_path_gridded}')\n",
    "# basin_avg_tws_gapped.to_pandas().to_csv(f'{save_path_basin}', sep=',',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thank you for reading!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyshbundle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
